{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "UsageError: Line magic function `%matplotlib_inline` not found.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import IPython\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from shutil import copyfile\n",
    "from keras import optimizers, regularizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers import Dense, Activation, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import pywt\n",
    "\n",
    "import seaborn as sns  \n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n",
    "%matplotlib_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12859, 34)\n",
      "(12859,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('D:\\PDF\\8th_Sem\\BTP_works\\datacheck1.csv')\n",
    "data.head()\n",
    "\n",
    "X = data.drop('LABEL', axis = 1)\n",
    "y = data['LABEL']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, input_dim=34, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  \n",
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 100)               3500      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 12,141\n",
      "Trainable params: 12,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3067: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "Epoch 1/10\n",
      "12859/12859 [==============================] - ETA: 8:07 - loss: 0.6891 - acc: 0.140 - ETA: 36s - loss: -0.2446 - acc: 0.205 - ETA: 17s - loss: -5.5429 - acc: 0.191 - ETA: 11s - loss: -13.8904 - acc: 0.19 - ETA: 7s - loss: -18.6506 - acc: 0.1988 - ETA: 5s - loss: -22.0701 - acc: 0.195 - ETA: 4s - loss: -23.7406 - acc: 0.191 - ETA: 3s - loss: -25.1280 - acc: 0.190 - ETA: 2s - loss: -26.1492 - acc: 0.192 - ETA: 2s - loss: -26.7451 - acc: 0.194 - ETA: 1s - loss: -27.4008 - acc: 0.191 - ETA: 1s - loss: -27.9625 - acc: 0.192 - ETA: 0s - loss: -28.3804 - acc: 0.192 - ETA: 0s - loss: -28.7554 - acc: 0.193 - ETA: 0s - loss: -29.0786 - acc: 0.192 - ETA: 0s - loss: -29.4593 - acc: 0.191 - 3s 253us/step - loss: -29.6959 - acc: 0.1906\n",
      "Epoch 2/10\n",
      "12859/12859 [==============================] - ETA: 0s - loss: -30.3577 - acc: 0.265 - ETA: 0s - loss: -33.6439 - acc: 0.203 - ETA: 0s - loss: -34.3955 - acc: 0.191 - ETA: 0s - loss: -34.1664 - acc: 0.193 - ETA: 0s - loss: -34.1701 - acc: 0.187 - ETA: 0s - loss: -33.8514 - acc: 0.188 - ETA: 0s - loss: -33.5814 - acc: 0.191 - ETA: 0s - loss: -33.5135 - acc: 0.191 - ETA: 0s - loss: -33.3959 - acc: 0.194 - ETA: 0s - loss: -33.4957 - acc: 0.191 - ETA: 0s - loss: -33.5597 - acc: 0.190 - ETA: 0s - loss: -33.5432 - acc: 0.192 - ETA: 0s - loss: -33.5433 - acc: 0.192 - ETA: 0s - loss: -33.5618 - acc: 0.191 - ETA: 0s - loss: -33.6267 - acc: 0.190 - ETA: 0s - loss: -33.6071 - acc: 0.190 - ETA: 0s - loss: -33.6190 - acc: 0.189 - 1s 69us/step - loss: -33.4590 - acc: 0.1912\n",
      "Epoch 3/10\n",
      "12859/12859 [==============================] - ETA: 0s - loss: -28.1483 - acc: 0.312 - ETA: 0s - loss: -33.8201 - acc: 0.186 - ETA: 0s - loss: -33.7913 - acc: 0.193 - ETA: 0s - loss: -33.4241 - acc: 0.191 - ETA: 0s - loss: -33.5614 - acc: 0.191 - ETA: 0s - loss: -33.6630 - acc: 0.189 - ETA: 0s - loss: -33.6093 - acc: 0.190 - ETA: 0s - loss: -33.6148 - acc: 0.191 - ETA: 0s - loss: -33.4033 - acc: 0.195 - ETA: 0s - loss: -33.4008 - acc: 0.194 - ETA: 0s - loss: -33.4049 - acc: 0.194 - ETA: 0s - loss: -33.5926 - acc: 0.190 - ETA: 0s - loss: -33.5900 - acc: 0.190 - ETA: 0s - loss: -33.5465 - acc: 0.191 - ETA: 0s - loss: -33.4663 - acc: 0.192 - ETA: 0s - loss: -33.4902 - acc: 0.191 - ETA: 0s - loss: -33.4994 - acc: 0.191 - 1s 68us/step - loss: -33.4605 - acc: 0.1912\n",
      "Epoch 4/10\n",
      "12859/12859 [==============================] - ETA: 0s - loss: -33.3794 - acc: 0.187 - ETA: 0s - loss: -33.5039 - acc: 0.191 - ETA: 0s - loss: -33.7580 - acc: 0.186 - ETA: 0s - loss: -34.1988 - acc: 0.181 - ETA: 0s - loss: -33.9362 - acc: 0.189 - ETA: 0s - loss: -33.8503 - acc: 0.188 - ETA: 0s - loss: -34.0054 - acc: 0.185 - ETA: 0s - loss: -33.9492 - acc: 0.186 - ETA: 0s - loss: -33.7845 - acc: 0.188 - ETA: 0s - loss: -33.7779 - acc: 0.188 - ETA: 0s - loss: -33.8204 - acc: 0.187 - ETA: 0s - loss: -33.7200 - acc: 0.189 - ETA: 0s - loss: -33.6836 - acc: 0.189 - ETA: 0s - loss: -33.5351 - acc: 0.191 - ETA: 0s - loss: -33.4932 - acc: 0.192 - ETA: 0s - loss: -33.4418 - acc: 0.191 - ETA: 0s - loss: -33.4895 - acc: 0.190 - 1s 69us/step - loss: -33.4604 - acc: 0.1912\n",
      "Epoch 5/10\n",
      "12859/12859 [==============================] - ETA: 0s - loss: -34.8740 - acc: 0.218 - ETA: 0s - loss: -32.0381 - acc: 0.210 - ETA: 0s - loss: -32.6417 - acc: 0.201 - ETA: 0s - loss: -32.5171 - acc: 0.204 - ETA: 0s - loss: -32.8524 - acc: 0.197 - ETA: 0s - loss: -32.8888 - acc: 0.198 - ETA: 0s - loss: -33.0440 - acc: 0.197 - ETA: 0s - loss: -33.1659 - acc: 0.197 - ETA: 0s - loss: -33.3794 - acc: 0.194 - ETA: 0s - loss: -33.4459 - acc: 0.192 - ETA: 0s - loss: -33.4991 - acc: 0.191 - ETA: 0s - loss: -33.4612 - acc: 0.192 - ETA: 0s - loss: -33.5851 - acc: 0.190 - ETA: 0s - loss: -33.5365 - acc: 0.191 - ETA: 0s - loss: -33.5087 - acc: 0.191 - ETA: 0s - loss: -33.4116 - acc: 0.192 - 1s 65us/step - loss: -33.4605 - acc: 0.1912\n",
      "Epoch 6/10\n",
      "12859/12859 [==============================] - ETA: 0s - loss: -31.3866 - acc: 0.203 - ETA: 0s - loss: -33.5710 - acc: 0.185 - ETA: 0s - loss: -33.0728 - acc: 0.193 - ETA: 0s - loss: -33.6221 - acc: 0.187 - ETA: 0s - loss: -33.8297 - acc: 0.187 - ETA: 0s - loss: -33.7626 - acc: 0.187 - ETA: 0s - loss: -33.5550 - acc: 0.187 - ETA: 0s - loss: -33.6214 - acc: 0.184 - ETA: 0s - loss: -33.7722 - acc: 0.184 - ETA: 0s - loss: -33.6911 - acc: 0.185 - ETA: 0s - loss: -33.5969 - acc: 0.187 - ETA: 0s - loss: -33.6051 - acc: 0.187 - ETA: 0s - loss: -33.6257 - acc: 0.187 - ETA: 0s - loss: -33.5327 - acc: 0.188 - ETA: 0s - loss: -33.5179 - acc: 0.188 - ETA: 0s - loss: -33.5286 - acc: 0.188 - ETA: 0s - loss: -33.4608 - acc: 0.190 - 1s 68us/step - loss: -33.4598 - acc: 0.1912\n",
      "Epoch 7/10\n",
      "12859/12859 [==============================] - ETA: 0s - loss: -34.8740 - acc: 0.125 - ETA: 0s - loss: -34.2416 - acc: 0.193 - ETA: 0s - loss: -33.4175 - acc: 0.203 - ETA: 0s - loss: -33.0046 - acc: 0.203 - ETA: 0s - loss: -33.0806 - acc: 0.197 - ETA: 0s - loss: -33.0245 - acc: 0.197 - ETA: 0s - loss: -33.1232 - acc: 0.198 - ETA: 0s - loss: -33.2986 - acc: 0.193 - ETA: 0s - loss: -33.1498 - acc: 0.195 - ETA: 0s - loss: -33.1616 - acc: 0.194 - ETA: 0s - loss: -33.1818 - acc: 0.192 - ETA: 0s - loss: -33.4133 - acc: 0.191 - ETA: 0s - loss: -33.4957 - acc: 0.190 - ETA: 0s - loss: -33.4269 - acc: 0.191 - ETA: 0s - loss: -33.4662 - acc: 0.190 - ETA: 0s - loss: -33.4456 - acc: 0.191 - ETA: 0s - loss: -33.3831 - acc: 0.192 - 1s 69us/step - loss: -33.4603 - acc: 0.1912\n",
      "Epoch 8/10\n",
      "12859/12859 [==============================] - ETA: 0s - loss: -31.3866 - acc: 0.234 - ETA: 0s - loss: -33.3794 - acc: 0.196 - ETA: 0s - loss: -33.7207 - acc: 0.186 - ETA: 0s - loss: -33.7592 - acc: 0.185 - ETA: 0s - loss: -33.6520 - acc: 0.187 - ETA: 0s - loss: -33.4545 - acc: 0.188 - ETA: 0s - loss: -33.2941 - acc: 0.192 - ETA: 0s - loss: -33.2518 - acc: 0.192 - ETA: 0s - loss: -33.2642 - acc: 0.192 - ETA: 0s - loss: -33.2318 - acc: 0.194 - ETA: 0s - loss: -33.3110 - acc: 0.194 - ETA: 0s - loss: -33.3956 - acc: 0.192 - ETA: 0s - loss: -33.3923 - acc: 0.192 - ETA: 0s - loss: -33.4980 - acc: 0.192 - ETA: 0s - loss: -33.5382 - acc: 0.190 - ETA: 0s - loss: -33.4960 - acc: 0.192 - ETA: 0s - loss: -33.4892 - acc: 0.191 - ETA: 0s - loss: -33.4445 - acc: 0.191 - 1s 70us/step - loss: -33.4605 - acc: 0.1912\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12859/12859 [==============================] - ETA: 1s - loss: -34.3758 - acc: 0.250 - ETA: 0s - loss: -34.3949 - acc: 0.177 - ETA: 0s - loss: -34.8221 - acc: 0.178 - ETA: 0s - loss: -34.0930 - acc: 0.190 - ETA: 0s - loss: -33.8327 - acc: 0.187 - ETA: 0s - loss: -33.9606 - acc: 0.190 - ETA: 0s - loss: -33.8546 - acc: 0.190 - ETA: 0s - loss: -33.9028 - acc: 0.189 - ETA: 0s - loss: -33.8971 - acc: 0.190 - ETA: 0s - loss: -33.8407 - acc: 0.189 - ETA: 0s - loss: -33.8114 - acc: 0.190 - ETA: 0s - loss: -33.7468 - acc: 0.189 - ETA: 0s - loss: -33.6980 - acc: 0.190 - ETA: 0s - loss: -33.6553 - acc: 0.189 - ETA: 0s - loss: -33.5814 - acc: 0.190 - ETA: 0s - loss: -33.5381 - acc: 0.190 - 1s 63us/step - loss: -33.4605 - acc: 0.1912\n",
      "Epoch 10/10\n",
      "12859/12859 [==============================] - ETA: 1s - loss: -29.1447 - acc: 0.281 - ETA: 0s - loss: -33.7530 - acc: 0.196 - ETA: 0s - loss: -33.7023 - acc: 0.192 - ETA: 0s - loss: -33.3794 - acc: 0.193 - ETA: 0s - loss: -33.5901 - acc: 0.188 - ETA: 0s - loss: -33.4943 - acc: 0.191 - ETA: 0s - loss: -33.5135 - acc: 0.191 - ETA: 0s - loss: -33.4605 - acc: 0.189 - ETA: 0s - loss: -33.5288 - acc: 0.188 - ETA: 0s - loss: -33.4871 - acc: 0.189 - ETA: 0s - loss: -33.5935 - acc: 0.189 - ETA: 0s - loss: -33.6077 - acc: 0.189 - ETA: 0s - loss: -33.4856 - acc: 0.191 - ETA: 0s - loss: -33.4489 - acc: 0.192 - ETA: 0s - loss: -33.4382 - acc: 0.192 - ETA: 0s - loss: -33.4402 - acc: 0.191 - ETA: 0s - loss: -33.4226 - acc: 0.191 - 1s 68us/step - loss: -33.4605 - acc: 0.1912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x202e32054e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dim = 34\n",
    "\n",
    "\n",
    "def dense_net_model(activation_func = 'relu', init_type='normal', optimiser='adam', dropout_rate=0.35):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1\n",
    "    model.add(Dense(100, input_dim = n_dim, init = init_type, activation = activation_func))\n",
    "    \n",
    "    # Layer 2\n",
    "    model.add(Dense(50, init = init_type, activation = activation_func))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Layer 3\n",
    "    model.add(Dense(50, init = init_type, activation = activation_func))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Layer 4\n",
    "    model.add(Dense(20, init = init_type, activation = activation_func))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(1, init = init_type, activation = 'sigmoid'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    rmsprop = optimizers.RMSprop(lr=0.0009, decay = 1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = dense_net_model()\n",
    "model.summary()\n",
    "model_dir = 'Saved_Models/'\n",
    "model_name = 'Test_Models'\n",
    "model_folder = model_dir + model_name\n",
    "if not os.path.exists(model_folder):\n",
    "    os.mkdir(model_folder)\n",
    "plot_model(model, to_file= model_folder + '/' + model_name + '.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "model.fit(X, y, epochs = 10, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
